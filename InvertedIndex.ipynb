{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a simple inverted index using unigrams, bigrams ,trigrams and a positional inverted index of unigrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "from queue import PriorityQueue\n",
    "from nltk import word_tokenize, regexp_tokenize, ngrams, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from urllib.parse import urlsplit\n",
    "import xlsxwriter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  a) Implement a simple inverted indexer that consumes the corpus from HW2: Task 1 as input and produces an inverted index as an output. Your index will contain the document IDs and term frequency within the document. Term frequencies  are stored in these inverted lists: TERM Ã  (docID, tf), (docID, tf), ... A TERM is defined as a word n-gram, where n = 1, 2, and 3. Therefore, you will have three inverted indexes, one for each value of n. \n",
    "    b) Store the number of terms in each document in a separate data structure.\n",
    "    c) You may employ any concrete data structures convenient for the programming language\n",
    "    you are using, as long as you can write them to disk and read them back in when you want\n",
    "    to run some queries. \n",
    "    d) Generate a positional inverted index for unigrams with gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read documents in the corpus.\n",
    "For each document - (1) update term frequency (2) update inverted list for each term in the document\n",
    "\"\"\"\n",
    "fpath = '/users/ankithakumari/Documents/CS6200/HW3/BFS'\n",
    "files = os.listdir(fpath)\n",
    "\n",
    "# Create data structures for storing inverted lists\n",
    "docNum = 1\n",
    "unigram_index = {}\n",
    "bigram_index = {}\n",
    "trigram_index ={}\n",
    "unigram_tf = {}\n",
    "# Data structures to store term frequencies in each document\n",
    "doc_tf_unigram = {}\n",
    "doc_tf_bigram = {}\n",
    "doc_tf_trigram = {}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility function that builds the index for a document\n",
    "Input: The index data structure, document number, frequency distibution of tokens\n",
    "Action: Updates the index with document number and term frequency of each term within that document\n",
    "The index is a dictionary where each inverted list is as follows:\n",
    "{term1: [doc1 tf1 doc2 tf2 ...]}\n",
    "the document ids and term frequencies are stored as a list. even indices represent document id and odd indices\n",
    "represent term frequency in each of the document preceding it.\n",
    "\"\"\"\n",
    "def buildIndex(index_ds, doc, tf_dist):\n",
    "    for term, freq in tf_dist.items():\n",
    "        if term not in index_ds.keys():\n",
    "            index_ds[term] = [doc, freq]\n",
    "        else:\n",
    "            index_ds[term].append(doc)\n",
    "            index_ds[term].append(freq)\n",
    "            \n",
    "\"\"\"\n",
    "Build index with position of terms\n",
    "The inverted list has the following format: \n",
    "{term1: {doc_id1 : [pos_1, pos_2 ...],  doc_id2:[pos_1, pos_2..] .. }}\n",
    "\"\"\"\n",
    "            \n",
    "def buildIndexWithPos(index_ds, doc, terms):\n",
    "    for term in FreqDist(ngrams(terms, 1)):\n",
    "        if term not in index_ds.keys():\n",
    "            index_ds[term] = {doc:[]}                     #initialize dictionary\n",
    "        else:\n",
    "            index_ds[term][doc] = []\n",
    "    i = 1\n",
    "    for item in ngrams(terms,1):\n",
    "        index_ds[item][doc].append(i) \n",
    "        i += 1       # appends the word position for each term as encountered while parsing\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    # read tokens from file\n",
    "    f = open('BFS/'+file, 'r', encoding=\"utf-8\")\n",
    "    tokens = f.read().splitlines()\n",
    "    f.close()\n",
    "    # remove any words that start with hyphen\n",
    "    tokens =[token.lower() for token in tokens if (re.match(r'^[a-zA-Z0-9][ A-Za-z0-9-]*$', token))]\n",
    "    # create n-grams\n",
    "    unigram = ngrams(tokens, 1)\n",
    "    bigram = ngrams(tokens, 2)\n",
    "    trigram = ngrams(tokens, 3)\n",
    "    \n",
    "    # store term frequencies\n",
    "    doc_tf_unigram[docNum] = sum(FreqDist(unigram).values())\n",
    "    doc_tf_bigram[docNum] = sum(FreqDist(bigram).values())\n",
    "    doc_tf_trigram[docNum] = sum(FreqDist(trigram).values())\n",
    "    \n",
    "    # create inverted index\n",
    "    buildIndex(bigram_index, docNum, FreqDist(ngrams(tokens, 2)))\n",
    "    buildIndex(trigram_index, docNum, FreqDist(ngrams(tokens, 3)))\n",
    "    buildIndex(unigram_tf, docNum, FreqDist(ngrams(tokens, 1)))\n",
    "    buildIndexWithPos(unigram_index, docNum, tokens)     # create positional index for unigrams\n",
    "    # increment document Number\n",
    "    docNum += 1\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encode positions with differences between consecutive document ids\n",
    "for term in unigram_index.keys():              # for each term in the index\n",
    "    for doc in unigram_index[term].keys():     # for each document in the list\n",
    "        for i in reversed(range(1, len(unigram_index[term][doc]))):\n",
    "            unigram_index[term][doc][i] =unigram_index[term][doc][i] - unigram_index[term][doc][i-1] # save the difference\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For writing the inverted indexes to files\n",
    "workbook = xlsxwriter.Workbook('idx3.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "row = 0\n",
    "\n",
    "for term in trigram_index.keys():\n",
    "    col = 0\n",
    "    row += 1\n",
    "    worksheet.write(row, col, str(term).strip(\"()\"))\n",
    "    posting = trigram_index[term]\n",
    "    i = 0\n",
    "    while col < len(posting)/2:\n",
    "        col += 1\n",
    "        worksheet.write(row, col, str(posting[i]) + \":\" + str(posting[i+1]))\n",
    "        i += 2\n",
    "        \n",
    "\n",
    "workbook.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "workbook = xlsxwriter.Workbook('unigram_idx.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "row = 1\n",
    "for term in unigram_index.keys():\n",
    "    col = 0\n",
    "    worksheet.write(row, col, str(term).strip(\"()\"))\n",
    "    postings = unigram_index[term]\n",
    "    col += 1\n",
    "    for doc, positions in postings.items():\n",
    "        worksheet.write(row, col, doc)\n",
    "        worksheet.write(row, col+1, str(positions))\n",
    "        worksheet.write(row,col+2, len(positions))\n",
    "        row += 1\n",
    "workbook.close()   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "workbook = xlsxwriter.Workbook('Doc_TF_Table.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "row = 1\n",
    "col = 0\n",
    "for doc, tf in doc_tf_unigram.items():\n",
    "    worksheet.write(row, col, doc)\n",
    "    worksheet.write(row, col+1, tf)\n",
    "    row += 1\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2\n",
    "a) Implement a program that uses a positional inverted index in Task 1-d) above to retrieve\n",
    "the list of documents that contain a pair of terms within a proximity window k.\n",
    "b) Using the positional index created in Task 1-d) above:\n",
    "i. Find the list of documents that contain both space and mission within k = 6 and k\n",
    "= 12 unigram tokens of each other. [Here, with k tokens implies there are at most\n",
    "k tokens between the two terms, and the terms may appear in any order.]\n",
    "ii. Find the list of documents that contain both earth and orbit within k = 5 and k = 10\n",
    "unigram tokens of each other.\n",
    "Note 1: Terms are not case-sensitive.\n",
    "Note 2: You will produce 4 lists of Document IDs in total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decode the encoded positional indices\n",
    "def getposition(pos_list):\n",
    "    return [sum(pos_list[0:i]) for i in range(1, len(pos_list)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get list of documents that contain the words within a given proximity window\n",
    "def getDocument(word1, word2, k):\n",
    "    commons = set(unigram_index[word1].keys()) & set(unigram_index[word2].keys())  # get documents common to both words\n",
    "    inv_list = {}\n",
    "    res = []\n",
    "    inv_list[word1] = unigram_index[word1]        # get inverted list for word 1\n",
    "    inv_list[word2] = unigram_index[word2]        # get inverted list for word 2\n",
    "    for doc in commons:\n",
    "        pos1 = getposition(inv_list[word1][doc])    # get positions for word 1\n",
    "        pos2 = getposition(inv_list[word2][doc])    # get positions for word 2\n",
    "        for idx in pos1:\n",
    "            prox = range(idx-k, idx+k+1)              # for each position of word1 in doc check if word2 is within k positions\n",
    "            if len(set(prox) & set(pos2)):          \n",
    "                res.append(doc)                      # if word2 is within k positions append docid to the result list\n",
    "                break\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"space mission\"            # for query string 'space mission'\n",
    "word_list = []\n",
    "for token in ngrams(word_tokenize(query), 1):       # tokenize the query - for matching the index terms\n",
    "    word_list.append(token)\n",
    "doc1 = getDocument(word_list[0], word_list[1], 6)    # get document list within k = 6 positions\n",
    "doc2 = getDocument(word_list[0], word_list[1], 12)   # get document list within k = 12 positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"earth orbit\"                        # for query string 'earth orbit'\n",
    "word_list = []\n",
    "for token in ngrams(word_tokenize(query), 1):          # tokenize the query\n",
    "    word_list.append(token)\n",
    "doc3 = getDocument(word_list[0], word_list[1], 5)        # get document list within k = 5 positions\n",
    "doc4 = getDocument(word_list[0], word_list[1], 10)       # get document list within k = 10 positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the document lists to files\n",
    "with open(\"space_mission_12.txt\", 'w+') as myfile:\n",
    "    myfile.write(str(doc2))\n",
    "myfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (25 points): Compute Corpus Statistics; Propose Stop Lists\n",
    "a) For each inverted index in Task 1-a), generate a term frequency table comprising of two\n",
    "columns: term and term frequency (for the corpus).\n",
    "Sort the table from most to least frequent.\n",
    "b) For each inverted index in Task 1-a), generate a document frequency table comprising of\n",
    "three columns: term, docIDs, and document frequency. Sort lexicographically based on\n",
    "term.\n",
    "Note: For tasks 3-a) and 3-b), you will generate six tables in total: two tables for word\n",
    "unigrams, two tables for word bigrams, and two tables for word trigrams.\n",
    "c) Generate three stop lists, one per index from Task 1-a). How would you choose your\n",
    "cutoff values? Briefly justify your choice and comment on the stop listsâ contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function is used to compute the term frequency for the corpus for a given inverted index\n",
    "\"\"\"\n",
    "def termFrequency(index_ds):\n",
    "    res = {}\n",
    "    for term in index_ds.keys():      # for each term in the inverted index\n",
    "        tot_freq = 0\n",
    "        posting = index_ds[term]      # get the list of document ids and term frequency\n",
    "        for i in range(len(posting)):\n",
    "            if i%2 != 0:                 # since frequencies are stored in odd index positions\n",
    "                tot_freq += posting[i]     # add the frequency to total frequency\n",
    "        res[term] = tot_freq               # append to result\n",
    "    return res\n",
    "\n",
    "unigram_freq = termFrequency(unigram_tf)      # get term frequency for unigrams\n",
    "bigram_freq = termFrequency(bigram_index)    # get term frequency for bigrams\n",
    "trigram_freq = termFrequency(trigram_index)    # get term frequency for trigrams\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort the term frequencies in descending order\n",
    "sorted_unigrams = [(key, unigram_freq[key]) for key in sorted(unigram_freq, key=unigram_freq.__getitem__, reverse=True)]\n",
    "sorted_bigrams = [(key, bigram_freq[key]) for key in sorted(bigram_freq, key=bigram_freq.__getitem__, reverse=True)]\n",
    "sorted_trigrams = [(key, trigram_freq[key]) for key in sorted(trigram_freq, key=trigram_freq.__getitem__, reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Used to dump the data structures to text files\n",
    "f = open(\"trigram_term_frequency.txt\", \"w+\") \n",
    "f.write(str(sorted_trigrams))\n",
    "f.close()\n",
    "f = open(\"unigram_term_frequency.txt\", \"w+\")\n",
    "f.write(str(sorted_unigrams))\n",
    "f.close()\n",
    "f = open(\"bigram_term_frequency.txt\", \"w+\")\n",
    "f.write(str(sorted_bigrams))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function is used to retrieve the document list for each term \n",
    "\"\"\"\n",
    "def documentFrequency(index_ds):\n",
    "    res = {}\n",
    "    for term in index_ds.keys():      # for each term in the index\n",
    "        doc_ids = []\n",
    "        posting = index_ds[term]          # get the inverted list\n",
    "        for i, item in enumerate(posting):      # for each item in the posting\n",
    "            if i%2 == 0:\n",
    "                doc_ids.append(item)           # retrieve document id from even index positions\n",
    "        res[str(term).strip(\"\\'(),\\'\")] = doc_ids   # save the term and document list, strip the brackets from the term to aid sorting\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "bigram_doc = documentFrequency(bigram_index)            # generate table for bigrams\n",
    "trigram_doc = documentFrequency(trigram_index)          # generate table for trigrams\n",
    "unigram_doc = documentFrequency(unigram_tf)           # generate table for unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort the terms lexicographically\n",
    "unigram_doc = [(key, unigram_doc[key]) for key in sorted(unigram_doc.keys())]\n",
    "\n",
    "bigram_doc = [(key, bigram_doc[key]) for key in sorted(bigram_doc.keys())]\n",
    "\n",
    "trigram_doc = [(key, trigram_doc[key]) for key in sorted(trigram_doc.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the tables to files\n",
    "workbook = xlsxwriter.Workbook('Trigram_Document_Frequency.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "row = 1\n",
    "col = 0\n",
    "for term in trigram_doc:\n",
    "    worksheet.write(row, col, term[0])\n",
    "    worksheet.write(row, col+1, str(term[1]))\n",
    "    worksheet.write(row, col+2, len(term[1]))\n",
    "    row += 1\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(of,)</th>\n",
       "      <td>0.003017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the,)</th>\n",
       "      <td>0.004024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.009077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(and,)</th>\n",
       "      <td>0.009077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in,)</th>\n",
       "      <td>0.010091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(to,)</th>\n",
       "      <td>0.013138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(as,)</th>\n",
       "      <td>0.027483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.027483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(with,)</th>\n",
       "      <td>0.033694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(by,)</th>\n",
       "      <td>0.036814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(for,)</th>\n",
       "      <td>0.036814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(that,)</th>\n",
       "      <td>0.042036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(from,)</th>\n",
       "      <td>0.044132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(on,)</th>\n",
       "      <td>0.050448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(it,)</th>\n",
       "      <td>0.053621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(which,)</th>\n",
       "      <td>0.055742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(an,)</th>\n",
       "      <td>0.057867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(be,)</th>\n",
       "      <td>0.065342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(at,)</th>\n",
       "      <td>0.071794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.077202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(or,)</th>\n",
       "      <td>0.085917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(also,)</th>\n",
       "      <td>0.093605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(was,)</th>\n",
       "      <td>0.101352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(not,)</th>\n",
       "      <td>0.113650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(are,)</th>\n",
       "      <td>0.123825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(have,)</th>\n",
       "      <td>0.128381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(one,)</th>\n",
       "      <td>0.137556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(its,)</th>\n",
       "      <td>0.139862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(but,)</th>\n",
       "      <td>0.144492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(other,)</th>\n",
       "      <td>0.149143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(around,)</th>\n",
       "      <td>0.346969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(being,)</th>\n",
       "      <td>0.355528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(up,)</th>\n",
       "      <td>0.356962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(including,)</th>\n",
       "      <td>0.359835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(so,)</th>\n",
       "      <td>0.368506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(part,)</th>\n",
       "      <td>0.369958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(made,)</th>\n",
       "      <td>0.381654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(since,)</th>\n",
       "      <td>0.383126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(system,)</th>\n",
       "      <td>0.389035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(any,)</th>\n",
       "      <td>0.394978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(three,)</th>\n",
       "      <td>0.394978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(due,)</th>\n",
       "      <td>0.396470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(out,)</th>\n",
       "      <td>0.403960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(well,)</th>\n",
       "      <td>0.405465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(several,)</th>\n",
       "      <td>0.408482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(before,)</th>\n",
       "      <td>0.411507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(new,)</th>\n",
       "      <td>0.413024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(within,)</th>\n",
       "      <td>0.422170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(same,)</th>\n",
       "      <td>0.428315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(use,)</th>\n",
       "      <td>0.429857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(although,)</th>\n",
       "      <td>0.431401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(years,)</th>\n",
       "      <td>0.432948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(large,)</th>\n",
       "      <td>0.432948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(each,)</th>\n",
       "      <td>0.445409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(could,)</th>\n",
       "      <td>0.446978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(much,)</th>\n",
       "      <td>0.451698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(very,)</th>\n",
       "      <td>0.454858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(early,)</th>\n",
       "      <td>0.456441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(later,)</th>\n",
       "      <td>0.465996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(using,)</th>\n",
       "      <td>0.469201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0\n",
       "(of,)         0.003017\n",
       "(the,)        0.004024\n",
       "(a,)          0.009077\n",
       "(and,)        0.009077\n",
       "(in,)         0.010091\n",
       "(to,)         0.013138\n",
       "(as,)         0.027483\n",
       "(is,)         0.027483\n",
       "(with,)       0.033694\n",
       "(by,)         0.036814\n",
       "(for,)        0.036814\n",
       "(that,)       0.042036\n",
       "(from,)       0.044132\n",
       "(on,)         0.050448\n",
       "(it,)         0.053621\n",
       "(which,)      0.055742\n",
       "(an,)         0.057867\n",
       "(be,)         0.065342\n",
       "(at,)         0.071794\n",
       "(this,)       0.077202\n",
       "(or,)         0.085917\n",
       "(also,)       0.093605\n",
       "(was,)        0.101352\n",
       "(not,)        0.113650\n",
       "(are,)        0.123825\n",
       "(have,)       0.128381\n",
       "(one,)        0.137556\n",
       "(its,)        0.139862\n",
       "(but,)        0.144492\n",
       "(other,)      0.149143\n",
       "...                ...\n",
       "(around,)     0.346969\n",
       "(being,)      0.355528\n",
       "(up,)         0.356962\n",
       "(including,)  0.359835\n",
       "(so,)         0.368506\n",
       "(part,)       0.369958\n",
       "(made,)       0.381654\n",
       "(since,)      0.383126\n",
       "(system,)     0.389035\n",
       "(any,)        0.394978\n",
       "(three,)      0.394978\n",
       "(due,)        0.396470\n",
       "(out,)        0.403960\n",
       "(well,)       0.405465\n",
       "(several,)    0.408482\n",
       "(before,)     0.411507\n",
       "(new,)        0.413024\n",
       "(within,)     0.422170\n",
       "(same,)       0.428315\n",
       "(use,)        0.429857\n",
       "(although,)   0.431401\n",
       "(years,)      0.432948\n",
       "(large,)      0.432948\n",
       "(each,)       0.445409\n",
       "(could,)      0.446978\n",
       "(much,)       0.451698\n",
       "(very,)       0.454858\n",
       "(early,)      0.456441\n",
       "(later,)      0.465996\n",
       "(using,)      0.469201\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns idf for a given posting since docs and term freq are in a single list, len(posting)/2 returns # of documents\n",
    "# containing the word\n",
    "def docfrequency(posting):\n",
    "    return np.log(len(doc_tf_unigram)/(len(posting)/2)) \n",
    "\n",
    "uni_df = {}\n",
    "for term in unigram_tf.keys():\n",
    "    posting = unigram_tf[term]      # get the postings for each term\n",
    "    idf = docfrequency(posting)     # compute idf \n",
    "    uni_df[term] = idf\n",
    "        \n",
    "    \n",
    "uni_dataframe = pd.DataFrame.from_dict(uni_df, orient='index')         # convert the dictionary to data frame\n",
    "\n",
    "uni_dataframe.sort_values(by = 0).head(100)                # sort the idf scores and select top 100 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uni_stop_list = uni_dataframe.sort_values(by=0).head(100).index  # save the top 100 terms in stop list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(of, the)</th>\n",
       "      <td>0.018238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in, the)</th>\n",
       "      <td>0.044132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(to, the)</th>\n",
       "      <td>0.057867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(and, the)</th>\n",
       "      <td>0.103577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(from, the)</th>\n",
       "      <td>0.135254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(on, the)</th>\n",
       "      <td>0.137556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(by, the)</th>\n",
       "      <td>0.142174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(with, the)</th>\n",
       "      <td>0.157335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(of, a)</th>\n",
       "      <td>0.178714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(to, be)</th>\n",
       "      <td>0.183527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(as, the)</th>\n",
       "      <td>0.201787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(for, the)</th>\n",
       "      <td>0.206713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(as, a)</th>\n",
       "      <td>0.226664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(at, the)</th>\n",
       "      <td>0.232981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, first)</th>\n",
       "      <td>0.247021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in, a)</th>\n",
       "      <td>0.258656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, a)</th>\n",
       "      <td>0.273064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(that, the)</th>\n",
       "      <td>0.277030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, the)</th>\n",
       "      <td>0.290363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(such, as)</th>\n",
       "      <td>0.295747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(it, is)</th>\n",
       "      <td>0.298449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(with, a)</th>\n",
       "      <td>0.324496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(to, a)</th>\n",
       "      <td>0.381654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(due, to)</th>\n",
       "      <td>0.414542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(have, been)</th>\n",
       "      <td>0.428315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(by, a)</th>\n",
       "      <td>0.434497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(part, of)</th>\n",
       "      <td>0.439159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(it, was)</th>\n",
       "      <td>0.439159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, same)</th>\n",
       "      <td>0.442279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(one, of)</th>\n",
       "      <td>0.446978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, also)</th>\n",
       "      <td>0.734137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(over, the)</th>\n",
       "      <td>0.744652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, other)</th>\n",
       "      <td>0.746768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(because, of)</th>\n",
       "      <td>0.748889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(around, the)</th>\n",
       "      <td>0.751015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(would, be)</th>\n",
       "      <td>0.759562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, time)</th>\n",
       "      <td>0.768182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(some, of)</th>\n",
       "      <td>0.796724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(are, the)</th>\n",
       "      <td>0.796724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, second)</th>\n",
       "      <td>0.798954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, two)</th>\n",
       "      <td>0.801189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, an)</th>\n",
       "      <td>0.812437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(as, an)</th>\n",
       "      <td>0.816973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(that, is)</th>\n",
       "      <td>0.823814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(within, the)</th>\n",
       "      <td>0.826105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in, an)</th>\n",
       "      <td>0.839962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(at, least)</th>\n",
       "      <td>0.842290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in, this)</th>\n",
       "      <td>0.846963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in, which)</th>\n",
       "      <td>0.851658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(up, to)</th>\n",
       "      <td>0.854014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, few)</th>\n",
       "      <td>0.856375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(use, of)</th>\n",
       "      <td>0.858742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, large)</th>\n",
       "      <td>0.858742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(but, the)</th>\n",
       "      <td>0.861114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(most, of)</th>\n",
       "      <td>0.863493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(that, it)</th>\n",
       "      <td>0.865876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(while, the)</th>\n",
       "      <td>0.868266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(since, the)</th>\n",
       "      <td>0.868266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(has, a)</th>\n",
       "      <td>0.870661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(of, their)</th>\n",
       "      <td>0.873062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "(of, the)      0.018238\n",
       "(in, the)      0.044132\n",
       "(to, the)      0.057867\n",
       "(and, the)     0.103577\n",
       "(from, the)    0.135254\n",
       "(on, the)      0.137556\n",
       "(by, the)      0.142174\n",
       "(with, the)    0.157335\n",
       "(of, a)        0.178714\n",
       "(to, be)       0.183527\n",
       "(as, the)      0.201787\n",
       "(for, the)     0.206713\n",
       "(as, a)        0.226664\n",
       "(at, the)      0.232981\n",
       "(the, first)   0.247021\n",
       "(in, a)        0.258656\n",
       "(is, a)        0.273064\n",
       "(that, the)    0.277030\n",
       "(is, the)      0.290363\n",
       "(such, as)     0.295747\n",
       "(it, is)       0.298449\n",
       "(with, a)      0.324496\n",
       "(to, a)        0.381654\n",
       "(due, to)      0.414542\n",
       "(have, been)   0.428315\n",
       "(by, a)        0.434497\n",
       "(part, of)     0.439159\n",
       "(it, was)      0.439159\n",
       "(the, same)    0.442279\n",
       "(one, of)      0.446978\n",
       "...                 ...\n",
       "(is, also)     0.734137\n",
       "(over, the)    0.744652\n",
       "(the, other)   0.746768\n",
       "(because, of)  0.748889\n",
       "(around, the)  0.751015\n",
       "(would, be)    0.759562\n",
       "(the, time)    0.768182\n",
       "(some, of)     0.796724\n",
       "(are, the)     0.796724\n",
       "(the, second)  0.798954\n",
       "(the, two)     0.801189\n",
       "(is, an)       0.812437\n",
       "(as, an)       0.816973\n",
       "(that, is)     0.823814\n",
       "(within, the)  0.826105\n",
       "(in, an)       0.839962\n",
       "(at, least)    0.842290\n",
       "(in, this)     0.846963\n",
       "(in, which)    0.851658\n",
       "(up, to)       0.854014\n",
       "(a, few)       0.856375\n",
       "(use, of)      0.858742\n",
       "(a, large)     0.858742\n",
       "(but, the)     0.861114\n",
       "(most, of)     0.863493\n",
       "(that, it)     0.865876\n",
       "(while, the)   0.868266\n",
       "(since, the)   0.868266\n",
       "(has, a)       0.870661\n",
       "(of, their)    0.873062\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the stop list for bigrams\n",
    "bi_df = {}\n",
    "for term in bigram_index.keys():\n",
    "    term_tf = {}\n",
    "    posting = bigram_index[term]\n",
    "    idf = docfrequency(term, posting)\n",
    "    bi_df[term] = idf\n",
    "        \n",
    "    \n",
    "bi_dataframe = pd.DataFrame.from_dict(bi_df, orient='index')\n",
    "\n",
    "bi_dataframe.sort_values(by = 0).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bi_stop_list = bi_dataframe.sort_values(by=0).head(100).index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(one, of, the)</th>\n",
       "      <td>0.560870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(as, well, as)</th>\n",
       "      <td>0.616892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(part, of, the)</th>\n",
       "      <td>0.696161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(due, to, the)</th>\n",
       "      <td>0.817976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(such, as, the)</th>\n",
       "      <td>0.834013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(known, as, the)</th>\n",
       "      <td>0.938604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, number, of)</th>\n",
       "      <td>1.029820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, end, of)</th>\n",
       "      <td>1.032633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(was, the, first)</th>\n",
       "      <td>1.067020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(as, a, result)</th>\n",
       "      <td>1.090620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, united, states)</th>\n",
       "      <td>1.093610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(most, of, the)</th>\n",
       "      <td>1.127098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(some, of, the)</th>\n",
       "      <td>1.127098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in, order, to)</th>\n",
       "      <td>1.161748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(referred, to, as)</th>\n",
       "      <td>1.234870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(end, of, the)</th>\n",
       "      <td>1.252262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, use, of)</th>\n",
       "      <td>1.259304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(according, to, the)</th>\n",
       "      <td>1.266396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(because, of, the)</th>\n",
       "      <td>1.295279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(at, the, time)</th>\n",
       "      <td>1.310039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(based, on, the)</th>\n",
       "      <td>1.313764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in, the, early)</th>\n",
       "      <td>1.317502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, development, of)</th>\n",
       "      <td>1.321254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(to, be, the)</th>\n",
       "      <td>1.332597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, result, of)</th>\n",
       "      <td>1.347923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(as, part, of)</th>\n",
       "      <td>1.347923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(to, be, a)</th>\n",
       "      <td>1.379298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, series, of)</th>\n",
       "      <td>1.395362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in, addition, to)</th>\n",
       "      <td>1.407583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(of, the, first)</th>\n",
       "      <td>1.424112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, known, as)</th>\n",
       "      <td>1.722967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, rest, of)</th>\n",
       "      <td>1.722967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in, which, the)</th>\n",
       "      <td>1.722967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(can, not, be)</th>\n",
       "      <td>1.739965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in, the, form)</th>\n",
       "      <td>1.739965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, amount, of)</th>\n",
       "      <td>1.745695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in, the, united)</th>\n",
       "      <td>1.745695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(all, of, the)</th>\n",
       "      <td>1.745695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(on, the, other)</th>\n",
       "      <td>1.751459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, majority, of)</th>\n",
       "      <td>1.757256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, 20th, century)</th>\n",
       "      <td>1.763087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(thought, to, be)</th>\n",
       "      <td>1.763087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(at, the, end)</th>\n",
       "      <td>1.774852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, 19th, century)</th>\n",
       "      <td>1.811001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(it, is, a)</th>\n",
       "      <td>1.811001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(it, was, the)</th>\n",
       "      <td>1.817154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, result, of)</th>\n",
       "      <td>1.817154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(of, the, sun)</th>\n",
       "      <td>1.823346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(became, the, first)</th>\n",
       "      <td>1.835847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, formation, of)</th>\n",
       "      <td>1.842156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(in, the, first)</th>\n",
       "      <td>1.842156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(of, the, world)</th>\n",
       "      <td>1.842156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(can, be, used)</th>\n",
       "      <td>1.854895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, the, most)</th>\n",
       "      <td>1.854895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, fact, that)</th>\n",
       "      <td>1.854895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, size, of)</th>\n",
       "      <td>1.854895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(from, the, sun)</th>\n",
       "      <td>1.867798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(similar, to, the)</th>\n",
       "      <td>1.867798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(away, from, the)</th>\n",
       "      <td>1.874313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(the, center, of)</th>\n",
       "      <td>1.880870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0\n",
       "(one, of, the)          0.560870\n",
       "(as, well, as)          0.616892\n",
       "(part, of, the)         0.696161\n",
       "(due, to, the)          0.817976\n",
       "(such, as, the)         0.834013\n",
       "(known, as, the)        0.938604\n",
       "(a, number, of)         1.029820\n",
       "(the, end, of)          1.032633\n",
       "(was, the, first)       1.067020\n",
       "(as, a, result)         1.090620\n",
       "(the, united, states)   1.093610\n",
       "(most, of, the)         1.127098\n",
       "(some, of, the)         1.127098\n",
       "(in, order, to)         1.161748\n",
       "(referred, to, as)      1.234870\n",
       "(end, of, the)          1.252262\n",
       "(the, use, of)          1.259304\n",
       "(according, to, the)    1.266396\n",
       "(because, of, the)      1.295279\n",
       "(at, the, time)         1.310039\n",
       "(based, on, the)        1.313764\n",
       "(in, the, early)        1.317502\n",
       "(the, development, of)  1.321254\n",
       "(to, be, the)           1.332597\n",
       "(a, result, of)         1.347923\n",
       "(as, part, of)          1.347923\n",
       "(to, be, a)             1.379298\n",
       "(a, series, of)         1.395362\n",
       "(in, addition, to)      1.407583\n",
       "(of, the, first)        1.424112\n",
       "...                          ...\n",
       "(is, known, as)         1.722967\n",
       "(the, rest, of)         1.722967\n",
       "(in, which, the)        1.722967\n",
       "(can, not, be)          1.739965\n",
       "(in, the, form)         1.739965\n",
       "(the, amount, of)       1.745695\n",
       "(in, the, united)       1.745695\n",
       "(all, of, the)          1.745695\n",
       "(on, the, other)        1.751459\n",
       "(the, majority, of)     1.757256\n",
       "(the, 20th, century)    1.763087\n",
       "(thought, to, be)       1.763087\n",
       "(at, the, end)          1.774852\n",
       "(the, 19th, century)    1.811001\n",
       "(it, is, a)             1.811001\n",
       "(it, was, the)          1.817154\n",
       "(the, result, of)       1.817154\n",
       "(of, the, sun)          1.823346\n",
       "(became, the, first)    1.835847\n",
       "(the, formation, of)    1.842156\n",
       "(in, the, first)        1.842156\n",
       "(of, the, world)        1.842156\n",
       "(can, be, used)         1.854895\n",
       "(is, the, most)         1.854895\n",
       "(the, fact, that)       1.854895\n",
       "(the, size, of)         1.854895\n",
       "(from, the, sun)        1.867798\n",
       "(similar, to, the)      1.867798\n",
       "(away, from, the)       1.874313\n",
       "(the, center, of)       1.880870\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the stop list for trigrams\n",
    "tri_df = {}\n",
    "for term in trigram_index.keys():\n",
    "    term_tf = {}\n",
    "    posting = trigram_index[term]\n",
    "    idf = docfrequency(term, posting)\n",
    "    tri_df[term] = idf\n",
    "        \n",
    "    \n",
    "tri_dataframe = pd.DataFrame.from_dict(tri_df, orient='index')\n",
    "\n",
    "tri_dataframe.sort_values(by = 0).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tri_stop_list = tri_dataframe.sort_values(by = 0).head(100).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the stop lists to file\n",
    "with open('bigram_stop_list.txt', 'w+') as myfile:\n",
    "    myfile.write(str(bi_stop_list))\n",
    "myfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
